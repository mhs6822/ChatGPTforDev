{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "vocab.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8oaGGhdmYKqt"
      },
      "source": [
        "# 패키지 설치\n",
        "pip 명령어로 의존성 있는 패키지를 설치합니다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t8TJkXkpDnSq",
        "outputId": "ab474a61-89db-4f44-fb42-2c1e3de7594e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "!pip install ratsnlp"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ratsnlp\n",
            "  Downloading ratsnlp-1.0.53-py3-none-any.whl.metadata (741 bytes)\n",
            "Collecting pytorch-lightning==1.6.1 (from ratsnlp)\n",
            "  Downloading pytorch_lightning-1.6.1-py3-none-any.whl.metadata (33 kB)\n",
            "\u001b[33mWARNING: Ignoring version 1.6.1 of pytorch-lightning since it has invalid metadata:\n",
            "Requested pytorch-lightning==1.6.1 from https://files.pythonhosted.org/packages/77/5f/7f0b36c036334cb416230a7909b54c9a128a38623d2e18e87170e2055cd9/pytorch_lightning-1.6.1-py3-none-any.whl (from ratsnlp) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n",
            "    torch (>=1.8.*)\n",
            "           ~~~~~~^\n",
            "Please use pip<24.1 if you need to use this version.\u001b[0m\u001b[33m\n",
            "\u001b[0mINFO: pip is looking at multiple versions of ratsnlp to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting ratsnlp\n",
            "  Downloading ratsnlp-1.0.52-py3-none-any.whl.metadata (760 bytes)\n",
            "Collecting pytorch-lightning==1.6.1 (from ratsnlp)\n",
            "  Using cached pytorch_lightning-1.6.1-py3-none-any.whl.metadata (33 kB)\n",
            "\u001b[33mWARNING: Ignoring version 1.6.1 of pytorch-lightning since it has invalid metadata:\n",
            "Requested pytorch-lightning==1.6.1 from https://files.pythonhosted.org/packages/77/5f/7f0b36c036334cb416230a7909b54c9a128a38623d2e18e87170e2055cd9/pytorch_lightning-1.6.1-py3-none-any.whl (from ratsnlp) has invalid metadata: .* suffix can only be used with `==` or `!=` operators\n",
            "    torch (>=1.8.*)\n",
            "           ~~~~~~^\n",
            "Please use pip<24.1 if you need to use this version.\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting ratsnlp\n",
            "  Downloading ratsnlp-1.0.51-py3-none-any.whl.metadata (760 bytes)\n",
            "Collecting pytorch-lightning==1.3.4 (from ratsnlp)\n",
            "  Downloading pytorch_lightning-1.3.4-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting transformers==4.10.0 (from ratsnlp)\n",
            "  Downloading transformers-4.10.0-py3-none-any.whl.metadata (51 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.6/51.6 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting Korpora>=0.2.0 (from ratsnlp)\n",
            "  Downloading Korpora-0.2.0-py3-none-any.whl.metadata (26 kB)\n",
            "Requirement already satisfied: flask>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ratsnlp) (3.1.1)\n",
            "Collecting flask-ngrok>=0.0.25 (from ratsnlp)\n",
            "  Downloading flask_ngrok-0.0.25-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting flask-cors>=3.0.10 (from ratsnlp)\n",
            "  Downloading flask_cors-6.0.1-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning==1.3.4->ratsnlp) (2.0.2)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning==1.3.4->ratsnlp) (2.6.0+cu124)\n",
            "Requirement already satisfied: future>=0.17.1 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning==1.3.4->ratsnlp) (1.0.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning==1.3.4->ratsnlp) (4.67.1)\n",
            "Collecting PyYAML<=5.4.1,>=5.1 (from pytorch-lightning==1.3.4->ratsnlp)\n",
            "  Downloading PyYAML-5.4.1.tar.gz (175 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.1/175.1 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m \u001b[32mGetting requirements to build wheel\u001b[0m did not run successfully.\n",
            "\u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LvPQbN7vs50o"
      },
      "source": [
        "# 구글 드라이브 연동하기\n",
        "\n",
        "이번 튜토리얼에서 구축할 어휘 집합을 저장해 둘 구글 드라이브를 연결합니다. 자신의 구글 계정에 적용됩니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpjMKSVWs6B2",
        "outputId": "418f875d-07c1-49a0-a76c-c11ec7ab4a48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive', force_remount=True)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFt_JLYoTKA6"
      },
      "source": [
        "## 말뭉치 다운로드 및 전처리\n",
        "\n",
        "오픈소스 파이썬 패키지 코포라(Korpora)를 활용해 BPE 수행 대상 말뭉치를 내려받고 전처리합니다. 실습용 말뭉치는 박은정 님이 공개하신 Naver Sentiment Movie Corpus(NSMC)입니다.\n",
        "\n",
        "다음을 수행해 데이터를 내려받아 `nsmc`라는 변수로 읽어들입니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bThW-2OrTHZW"
      },
      "source": [
        "from Korpora import Korpora\n",
        "nsmc = Korpora.load(\"nsmc\", force_download=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K0NCnzpRTdnJ"
      },
      "source": [
        "다음을 수행하면 NSMC에 포함된 영화 리뷰(순수 텍스트)들을 지정된 경로에 저장해 둡니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FSVDbnFUTiXx"
      },
      "source": [
        "import os\n",
        "def write_lines(path, lines):\n",
        "    with open(path, 'w', encoding='utf-8') as f:\n",
        "        for line in lines:\n",
        "            f.write(f'{line}\\n')\n",
        "\n",
        "write_lines(\"/content/train.txt\", nsmc.train.get_all_texts())\n",
        "write_lines(\"/content/test.txt\", nsmc.test.get_all_texts())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWboOauTTyXp"
      },
      "source": [
        "`train.txt`의 앞부분은 다음과 같이 생겼습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tup5LreLT4vE"
      },
      "source": [
        "!head train.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dMKHuGgyT8aw"
      },
      "source": [
        "`test.txt`의 앞부분은 다음과 같이 생겼습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nI5EpyyaT_fl"
      },
      "source": [
        "!head test.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y2JYqRGUFll"
      },
      "source": [
        "## GPT 토크나이저 구축\n",
        "\n",
        "GPT 계열 모델이 사용하는 토크나이저는 Byte-level Byte Pair Encoding(BBPE)입니다. 우선 어휘집합 구축 결과를 저장해둘 디렉토리를 자신의 구글 드라이브 계정 내 `내 드라이브/nlpbook/bbpe`로 만들어 둡니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMsp3GRVuGg7"
      },
      "source": [
        "import os\n",
        "os.makedirs(\"/gdrive/My Drive/nlpbook/bbpe\", exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2jwK4iwvB1l"
      },
      "source": [
        "다음을 수행하면 `nsmc` 데이터를 가지고 BBPE 어휘집합을 구축합니다. BBPE 어휘집합 구축에 시간이 걸리니 잠시 기다려주세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Rk2Ga65USFb"
      },
      "source": [
        "from tokenizers import ByteLevelBPETokenizer\n",
        "bytebpe_tokenizer = ByteLevelBPETokenizer()\n",
        "bytebpe_tokenizer.train(\n",
        "    files=[\"/content/train.txt\", \"/content/test.txt\"],\n",
        "    vocab_size=10000,\n",
        "    special_tokens=[\"[PAD]\"]\n",
        ")\n",
        "bytebpe_tokenizer.save_model(\"/gdrive/My Drive/nlpbook/bbpe\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wLq1JRVJUb7U"
      },
      "source": [
        "위의 코드 수행이 끝나면 자신의 구글 드라이브 경로(`/gdrive/My Drive/nlpbook/bbpe`)에 `vocab.json`과 `merges.txt`가 생성됩니다. 전자는 바이트 레벨 BPE의 어휘 집합이며 후자는 바이그램 쌍의 병합 우선순위입니다.\n",
        "\n",
        "`vocab.json`은 다음과 같이 생겼습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XZl8OKdsVIQg"
      },
      "source": [
        "!cat /gdrive/My\\ Drive/nlpbook/bbpe/vocab.json"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "etZsJ4SrVXnS"
      },
      "source": [
        "`merges.txt`의 앞부분은 다음과 같이 생겼습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzkJ31vQVbjl"
      },
      "source": [
        "!head /gdrive/My\\ Drive/nlpbook/bbpe/merges.txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wR-3V6zIVsPp"
      },
      "source": [
        "## BERT 토크나이저 구축\n",
        "\n",
        "BERT는 워드피스(wordpiece) 토크나이저를 사용합니다. 우선 어휘집합 구축 결과를 저장해둘 디렉토리를 자신의 구글 드라이브 계정 내 `내 드라이브/nlpbook/bbpe`로 만들어 둡니다.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aObMCetww3YU"
      },
      "source": [
        "import os\n",
        "os.makedirs(\"/gdrive/My Drive/nlpbook/wordpiece\", exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jzmRa-NPw3l_"
      },
      "source": [
        "다음을 수행하면 BERT 모델이 사용하는 워드피스 어휘집합을 구축할 수 있습니다. 워드피스 어휘집합 구축에 시간이 걸리니 잠시만 기다려주세요."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FdvUPuJoV3w3"
      },
      "source": [
        "from tokenizers import BertWordPieceTokenizer\n",
        "wordpiece_tokenizer = BertWordPieceTokenizer(lowercase=False)\n",
        "wordpiece_tokenizer.train(\n",
        "    files=[\"/content/train.txt\", \"/content/test.txt\"],\n",
        "    vocab_size=10000,\n",
        ")\n",
        "wordpiece_tokenizer.save_model(\"/gdrive/My Drive/nlpbook/wordpiece\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0S2EbdkWIdq"
      },
      "source": [
        "위의 코드 수행이 끝나면 자신의 구글 드라이브 경로(`/gdrive/My Drive/nlpbook/wordpiece`)에 `vocab.txt`가 생성됩니다. `vocab.txt`의 앞부분은 다음과 같이 생겼습니다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOWkywHaWOL6"
      },
      "source": [
        "!head /gdrive/My\\ Drive/nlpbook/wordpiece/vocab.txt"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}